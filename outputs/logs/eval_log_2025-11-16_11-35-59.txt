
======================================================================
HFF-NET ENHANCED EVALUATION WITH XAI + METRICS
======================================================================
Log file: ./outputs/logs/eval_log_2025-11-16_11-35-59.txt
Output directory: ./outputs

[Loading] Model from /home/manish/BrainTumorHFF/HFF/result/checkpoints/hff/brats20/all/hff-l=0.3-e=200-s=50-g=0.55-b=1-cw=15-w=3-100L-H-2025-11-14_12-23-36/best_Result2_et_0.8320_tc_0.9098_wt_0.8970.pth
[LOAD] Loading checkpoint: /home/manish/BrainTumorHFF/HFF/result/checkpoints/hff/brats20/all/hff-l=0.3-e=200-s=50-g=0.55-b=1-cw=15-w=3-100L-H-2025-11-14_12-23-36/best_Result2_et_0.8320_tc_0.9098_wt_0.8970.pth
✓ Checkpoint loaded and model moved to device
✓ Model loaded successfully
✓ XAI modules initialized
✓ Run output directory: outputs/xai_2025-11-16_11-36-03

[Loading] Data from /home/manish/BrainTumorHFF/HFF/data/brats20/final/0-test.txt
[*] Load /home/manish/BrainTumorHFF/HFF/data/brats20/final/0-test.txt, which contains 38 paired volume
modilities: ['flair_L', 't1_L', 't1ce_L', 't2_L', 'flair_H1', 'flair_H2', 'flair_H3', 'flair_H4', 't1_H1', 't1_H2', 't1_H3', 't1_H4', 't1ce_H1', 't1ce_H2', 't1ce_H3', 't1ce_H4', 't2_H1', 't2_H2', 't2_H3', 't2_H4']
[*] Load /home/manish/BrainTumorHFF/HFF/data/brats20/final/0-test.txt, which contains 38 paired volume
modilities: ['flair_L', 't1_L', 't1ce_L', 't2_L', 'flair_H1', 'flair_H2', 'flair_H3', 'flair_H4', 't1_H1', 't1_H2', 't1_H3', 't1_H4', 't1ce_H1', 't1ce_H2', 't1ce_H3', 't1ce_H4', 't2_H1', 't2_H2', 't2_H3', 't2_H4']
✓ Data loaded: 38 batches

[Starting] Evaluation (XAI + per-sample metrics)...
======================================================================
Evaluating:   0%|          | 0/38 [00:00<?, ?it/s]
[1] Getting primary prediction...
Error processing batch 0: CUDA out of memory. Tried to allocate 128.00 MiB (GPU 0; 23.70 GiB total capacity; 437.92 MiB already allocated; 11.56 MiB free; 476.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Evaluating:   3%|2         | 1/38 [00:04<02:35,  4.21s/it]Error processing batch 1: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 0; 23.70 GiB total capacity; 437.92 MiB already allocated; 11.56 MiB free; 476.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Evaluating:   5%|5         | 2/38 [00:04<01:05,  1.83s/it]Error processing batch 2: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 0; 23.70 GiB total capacity; 437.92 MiB already allocated; 11.56 MiB free; 476.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Evaluating:   8%|7         | 3/38 [00:04<00:37,  1.08s/it]Error processing batch 3: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 0; 23.70 GiB total capacity; 437.92 MiB already allocated; 11.56 MiB free; 476.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Evaluating:  11%|#         | 4/38 [00:04<00:24,  1.39it/s]Error processing batch 4: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 0; 23.70 GiB total capacity; 437.92 MiB already allocated; 11.56 MiB free; 476.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Evaluating:  13%|#3        | 5/38 [00:07<00:47,  1.43s/it]Error processing batch 5: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 0; 23.70 GiB total capacity; 437.92 MiB already allocated; 11.56 MiB free; 476.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Evaluating:  16%|#5        | 6/38 [00:07<00:33,  1.05s/it]Error processing batch 6: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 0; 23.70 GiB total capacity; 437.92 MiB already allocated; 11.56 MiB free; 476.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Evaluating:  18%|#8        | 7/38 [00:07<00:24,  1.28it/s]Error processing batch 7: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 0; 23.70 GiB total capacity; 437.92 MiB already allocated; 11.56 MiB free; 476.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Evaluating:  21%|##1       | 8/38 [00:08<00:18,  1.65it/s]Error processing batch 8: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 0; 23.70 GiB total capacity; 437.92 MiB already allocated; 11.56 MiB free; 476.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Evaluating:  24%|##3       | 9/38 [00:10<00:34,  1.20s/it]Error processing batch 9: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 0; 23.70 GiB total capacity; 437.92 MiB already allocated; 11.56 MiB free; 476.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Evaluating:  26%|##6       | 10/38 [00:11<00:28,  1.03s/it]Error processing batch 10: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 0; 23.70 GiB total capacity; 437.92 MiB already allocated; 11.56 MiB free; 476.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Evaluating:  29%|##8       | 11/38 [00:11<00:20,  1.31it/s]Error processing batch 11: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 0; 23.70 GiB total capacity; 437.92 MiB already allocated; 11.56 MiB free; 476.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Evaluating:  32%|###1      | 12/38 [00:12<00:18,  1.37it/s]Error processing batch 12: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 0; 23.70 GiB total capacity; 437.92 MiB already allocated; 11.56 MiB free; 476.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Evaluating:  34%|###4      | 13/38 [00:13<00:26,  1.04s/it]Error processing batch 13: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 0; 23.70 GiB total capacity; 437.92 MiB already allocated; 11.56 MiB free; 476.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Evaluating:  37%|###6      | 14/38 [00:15<00:26,  1.10s/it]Error processing batch 14: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 0; 23.70 GiB total capacity; 437.92 MiB already allocated; 11.56 MiB free; 476.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Evaluating:  39%|###9      | 15/38 [00:15<00:18,  1.23it/s]Error processing batch 15: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 0; 23.70 GiB total capacity; 437.92 MiB already allocated; 11.56 MiB free; 476.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Evaluating:  42%|####2     | 16/38 [00:16<00:18,  1.17it/s]Error processing batch 16: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 0; 23.70 GiB total capacity; 437.92 MiB already allocated; 11.56 MiB free; 476.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Evaluating:  45%|####4     | 17/38 [00:17<00:17,  1.17it/s]Error processing batch 17: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 0; 23.70 GiB total capacity; 437.92 MiB already allocated; 11.56 MiB free; 476.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Evaluating:  47%|####7     | 18/38 [00:18<00:21,  1.06s/it]Error processing batch 18: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 0; 23.70 GiB total capacity; 437.92 MiB already allocated; 11.56 MiB free; 476.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Evaluating:  50%|#####     | 19/38 [00:18<00:15,  1.26it/s]Error processing batch 19: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 0; 23.70 GiB total capacity; 437.92 MiB already allocated; 11.56 MiB free; 476.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Evaluating:  53%|#####2    | 20/38 [00:20<00:19,  1.09s/it]Error processing batch 20: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 0; 23.70 GiB total capacity; 437.92 MiB already allocated; 11.56 MiB free; 476.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Evaluating:  55%|#####5    | 21/38 [00:20<00:13,  1.23it/s]Error processing batch 21: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 0; 23.70 GiB total capacity; 437.92 MiB already allocated; 11.56 MiB free; 476.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Evaluating:  58%|#####7    | 22/38 [00:21<00:14,  1.07it/s]Error processing batch 22: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 0; 23.70 GiB total capacity; 437.92 MiB already allocated; 11.56 MiB free; 476.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Evaluating:  61%|######    | 23/38 [00:22<00:10,  1.42it/s]Error processing batch 23: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 0; 23.70 GiB total capacity; 437.92 MiB already allocated; 11.56 MiB free; 476.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Evaluating:  63%|######3   | 24/38 [00:24<00:17,  1.24s/it]Error processing batch 24: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 0; 23.70 GiB total capacity; 437.92 MiB already allocated; 11.56 MiB free; 476.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Evaluating:  66%|######5   | 25/38 [00:24<00:11,  1.11it/s]Error processing batch 25: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 0; 23.70 GiB total capacity; 437.92 MiB already allocated; 11.56 MiB free; 476.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Evaluating:  68%|######8   | 26/38 [00:25<00:09,  1.27it/s]Error processing batch 26: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 0; 23.70 GiB total capacity; 437.92 MiB already allocated; 11.56 MiB free; 476.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Evaluating:  71%|#######1  | 27/38 [00:25<00:06,  1.66it/s]Error processing batch 27: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 0; 23.70 GiB total capacity; 437.92 MiB already allocated; 11.56 MiB free; 476.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Evaluating:  74%|#######3  | 28/38 [00:28<00:14,  1.47s/it]Error processing batch 28: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 0; 23.70 GiB total capacity; 437.92 MiB already allocated; 11.56 MiB free; 476.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Evaluating:  76%|#######6  | 29/38 [00:29<00:09,  1.08s/it]Error processing batch 29: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 0; 23.70 GiB total capacity; 437.92 MiB already allocated; 11.56 MiB free; 476.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Evaluating:  79%|#######8  | 30/38 [00:29<00:06,  1.23it/s]Error processing batch 30: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 0; 23.70 GiB total capacity; 437.92 MiB already allocated; 11.56 MiB free; 476.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Evaluating:  82%|########1 | 31/38 [00:29<00:04,  1.62it/s]Error processing batch 31: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 0; 23.70 GiB total capacity; 437.92 MiB already allocated; 11.56 MiB free; 476.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Evaluating:  84%|########4 | 32/38 [00:32<00:08,  1.41s/it]Error processing batch 32: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 0; 23.70 GiB total capacity; 437.92 MiB already allocated; 11.56 MiB free; 476.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Evaluating:  87%|########6 | 33/38 [00:32<00:05,  1.02s/it]Error processing batch 33: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 0; 23.70 GiB total capacity; 437.92 MiB already allocated; 11.56 MiB free; 476.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Evaluating:  89%|########9 | 34/38 [00:33<00:03,  1.07it/s]Error processing batch 34: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 0; 23.70 GiB total capacity; 437.92 MiB already allocated; 11.56 MiB free; 476.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Evaluating:  92%|#########2| 35/38 [00:33<00:02,  1.42it/s]Error processing batch 35: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 0; 23.70 GiB total capacity; 437.92 MiB already allocated; 11.56 MiB free; 476.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Evaluating:  95%|#########4| 36/38 [00:36<00:02,  1.29s/it]Error processing batch 36: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 0; 23.70 GiB total capacity; 437.92 MiB already allocated; 11.56 MiB free; 476.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Evaluating:  97%|#########7| 37/38 [00:36<00:00,  1.07it/s]Error processing batch 37: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 0; 23.70 GiB total capacity; 437.92 MiB already allocated; 11.56 MiB free; 476.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Evaluating: 100%|##########| 38/38 [00:37<00:00,  1.07it/s]Evaluating: 100%|##########| 38/38 [00:37<00:00,  1.01it/s]


======================================================================
Running standard validation pass (for exact print_val_loss/print_val_eval output)...
======================================================================

[*] Load /home/manish/BrainTumorHFF/HFF/data/brats20/final/0-test.txt, which contains 38 paired volume
modilities: ['flair_L', 't1_L', 't1ce_L', 't2_L', 'flair_H1', 'flair_H2', 'flair_H3', 'flair_H4', 't1_H1', 't1_H2', 't1_H3', 't1_H4', 't1ce_H1', 't1ce_H2', 't1ce_H3', 't1ce_H4', 't2_H1', 't2_H2', 't2_H3', 't2_H4']
[*] Load /home/manish/BrainTumorHFF/HFF/data/brats20/final/0-test.txt, which contains 38 paired volume
modilities: ['flair_L', 't1_L', 't1ce_L', 't2_L', 'flair_H1', 'flair_H2', 'flair_H3', 'flair_H4', 't1_H1', 't1_H2', 't1_H3', 't1_H4', 't1ce_H1', 't1ce_H2', 't1ce_H3', 't1ce_H4', 't2_H1', 't2_H2', 't2_H3', 't2_H4']
Validation Pass (printing):   0%|          | 0/38 [00:00<?, ?it/s]Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7effe7067c70>
Traceback (most recent call last):
  File "/home/manish/.conda/envs/hff-xai/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1478, in __del__
    self._shutdown_workers()
  File "/home/manish/.conda/envs/hff-xai/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1442, in _shutdown_workers
    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)
  File "/home/manish/.conda/envs/hff-xai/lib/python3.10/multiprocessing/process.py", line 149, in join
    res = self._popen.wait(timeout)
  File "/home/manish/.conda/envs/hff-xai/lib/python3.10/multiprocessing/popen_fork.py", line 40, in wait
    if not wait([self.sentinel], timeout):
  File "/home/manish/.conda/envs/hff-xai/lib/python3.10/multiprocessing/connection.py", line 936, in wait
    ready = selector.select(timeout)
  File "/home/manish/.conda/envs/hff-xai/lib/python3.10/selectors.py", line 416, in select
    fd_event_list = self._selector.poll(timeout)
KeyboardInterrupt: 
Validation Pass (printing):   0%|          | 0/38 [00:07<?, ?it/s]
Traceback (most recent call last):
  File "/home/manish/BrainTumorHFF/HFF/eval_new.py", line 482, in <module>
    low = torch.cat(low_freq_inputs, dim=1)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 0; 23.70 GiB total capacity; 437.92 MiB already allocated; 11.56 MiB free; 476.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
