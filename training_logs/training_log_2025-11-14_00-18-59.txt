[*] Load /home/manish/BrainTumorHFF/HFF/data/brats20/final/0-train.txt, which contains 295 paired volume
modilities: ['flair_L', 't1_L', 't1ce_L', 't2_L', 'flair_H1', 'flair_H2', 'flair_H3', 'flair_H4', 't1_H1', 't1_H2', 't1_H3', 't1_H4', 't1ce_H1', 't1ce_H2', 't1ce_H3', 't1ce_H4', 't2_H1', 't2_H2', 't2_H3', 't2_H4'][*] Load /home/manish/BrainTumorHFF/HFF/data/brats20/final/0-train.txt, which contains 295 paired volume
modilities: ['flair_L', 't1_L', 't1ce_L', 't2_L', 'flair_H1', 'flair_H2', 'flair_H3', 'flair_H4', 't1_H1', 't1_H2', 't1_H3', 't1_H4', 't1ce_H1', 't1ce_H2', 't1ce_H3', 't1ce_H4', 't2_H1', 't2_H2', 't2_H3', 't2_H4']

[*] Load /home/manish/BrainTumorHFF/HFF/data/brats20/final/0-val.txt, which contains 36 paired volume
modilities: ['flair_L', 't1_L', 't1ce_L', 't2_L', 'flair_H1', 'flair_H2', 'flair_H3', 'flair_H4', 't1_H1', 't1_H2', 't1_H3', 't1_H4', 't1ce_H1', 't1ce_H2', 't1ce_H3', 't1ce_H4', 't2_H1', 't2_H2', 't2_H3', 't2_H4'][*] Load /home/manish/BrainTumorHFF/HFF/data/brats20/final/0-val.txt, which contains 36 paired volume
modilities: ['flair_L', 't1_L', 't1ce_L', 't2_L', 'flair_H1', 'flair_H2', 'flair_H3', 'flair_H4', 't1_H1', 't1_H2', 't1_H3', 't1_H4', 't1ce_H1', 't1ce_H2', 't1ce_H3', 't1ce_H4', 't2_H1', 't2_H2', 't2_H3', 't2_H4']

[34m[1mwandb[0m: W&B API key is configured. Use [1m`wandb login --relogin`[0m to force relogin
[34m[1mwandb[0m: setting up run wqshzjax (0.0s)
[Am[2K[34m[1mwandb[0m: setting up run wqshzjax (0.0s)
[Am[2K[34m[1mwandb[0m: setting up run wqshzjax (0.0s)
[Am[2K[34m[1mwandb[0m: setting up run wqshzjax (0.0s)
[Am[2K[34m[1mwandb[0m: setting up run wqshzjax (0.0s)
[Am[2K[34m[1mwandb[0m: Tracking run with wandb version 0.22.3
[34m[1mwandb[0m: Run data is saved locally in [35m[1m/home/manish/BrainTumorHFF/HFF/wandb/run-20251114_001906-wqshzjax[0m
[34m[1mwandb[0m: Run [1m`wandb offline`[0m to turn off syncing.
[34m[1mwandb[0m: Syncing run [33mgenial-smoke-16[0m
[34m[1mwandb[0m:  View project at [34m[4mhttps://wandb.ai/k7aditya-iiita-alumni-affairs/learning%20rate%3D0.3epochs%3D200-step_size%3D50-gamma%3D0.55weight%20between%20braches%3D15warmup%20epochs%3D3brats20_all[0m
[34m[1mwandb[0m:  View run at [34m[4mhttps://wandb.ai/k7aditya-iiita-alumni-affairs/learning%20rate%3D0.3epochs%3D200-step_size%3D50-gamma%3D0.55weight%20between%20braches%3D15warmup%20epochs%3D3brats20_all/runs/wqshzjax[0m
[*] Load /home/manish/BrainTumorHFF/HFF/data/brats20/final/0-train.txt, which contains 295 paired volume
modilities: ['flair_L', 't1_L', 't1ce_L', 't2_L', 'flair_H1', 'flair_H2', 'flair_H3', 'flair_H4', 't1_H1', 't1_H2', 't1_H3', 't1_H4', 't1ce_H1', 't1ce_H2', 't1ce_H3', 't1ce_H4', 't2_H1', 't2_H2', 't2_H3', 't2_H4']
[*] Load /home/manish/BrainTumorHFF/HFF/data/brats20/final/0-val.txt, which contains 36 paired volume
modilities: ['flair_L', 't1_L', 't1ce_L', 't2_L', 'flair_H1', 'flair_H2', 'flair_H3', 'flair_H4', 't1_H1', 't1_H2', 't1_H3', 't1_H4', 't1ce_H1', 't1ce_H2', 't1ce_H3', 't1ce_H4', 't2_H1', 't2_H2', 't2_H3', 't2_H4']
[RESUME] Loaded weights from: /home/manish/BrainTumorHFF/HFF/result/checkpoints/hff/brats20/all/hff-l=0.3-e=200-s=50-g=0.55-b=1-cw=15-w=3-100L-H-2025-11-13_13-25-39/best_Result2_et_0.7411_tc_0.7448_wt_0.7826.pth
Set dropout rate to 0.5000 (18 layers)
Set dropout rate to 0.3780 (18 layers)
Epoch 62 - Dropout rate set to 0.3780
Set dropout rate to 0.5000 (18 layers)
  0%|                                                                    | 0/74 [00:00<?, ?it/s]Set dropout rate to 0.5000 (18 layers)
Set dropout rate to 0.3780 (18 layers)
Set dropout rate to 0.3780 (18 layers)
  0%|                                                                    | 0/74 [00:12<?, ?it/s]
Traceback (most recent call last):
  File "/home/manish/BrainTumorHFF/HFF/train_multiGPU.py", line 441, in <module>
    low_freq_inputs = torch.cat(low_freq_inputs, dim=1)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 0; 23.70 GiB total capacity; 363.92 MiB already allocated; 31.56 MiB free; 382.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
