[*] Load /home/manish/BrainTumorHFF/HFF/data/brats20/final/0-train.txt, which contains 295 paired volume
modilities: ['flair_L', 't1_L', 't1ce_L', 't2_L', 'flair_H1', 'flair_H2', 'flair_H3', 'flair_H4', 't1_H1', 't1_H2', 't1_H3', 't1_H4', 't1ce_H1', 't1ce_H2', 't1ce_H3', 't1ce_H4', 't2_H1', 't2_H2', 't2_H3', 't2_H4']
[*] Load /home/manish/BrainTumorHFF/HFF/data/brats20/final/0-val.txt, which contains 36 paired volume
modilities: ['flair_L', 't1_L', 't1ce_L', 't2_L', 'flair_H1', 'flair_H2', 'flair_H3', 'flair_H4', 't1_H1', 't1_H2', 't1_H3', 't1_H4', 't1ce_H1', 't1ce_H2', 't1ce_H3', 't1ce_H4', 't2_H1', 't2_H2', 't2_H3', 't2_H4']
[*] Load /home/manish/BrainTumorHFF/HFF/data/brats20/final/0-train.txt, which contains 295 paired volume
modilities: ['flair_L', 't1_L', 't1ce_L', 't2_L', 'flair_H1', 'flair_H2', 'flair_H3', 'flair_H4', 't1_H1', 't1_H2', 't1_H3', 't1_H4', 't1ce_H1', 't1ce_H2', 't1ce_H3', 't1ce_H4', 't2_H1', 't2_H2', 't2_H3', 't2_H4']
[*] Load /home/manish/BrainTumorHFF/HFF/data/brats20/final/0-val.txt, which contains 36 paired volume
modilities: ['flair_L', 't1_L', 't1ce_L', 't2_L', 'flair_H1', 'flair_H2', 'flair_H3', 'flair_H4', 't1_H1', 't1_H2', 't1_H3', 't1_H4', 't1ce_H1', 't1ce_H2', 't1ce_H3', 't1ce_H4', 't2_H1', 't2_H2', 't2_H3', 't2_H4']
[34m[1mwandb[0m: Currently logged in as: [33mk7aditya[0m ([33mk7aditya-iiita-alumni-affairs[0m) to [32mhttps://api.wandb.ai[0m. Use [1m`wandb login --relogin`[0m to force relogin
[34m[1mwandb[0m: setting up run eoqx1hgu (0.0s)
[Am[2K[34m[1mwandb[0m: setting up run eoqx1hgu (0.0s)
[Am[2K[34m[1mwandb[0m: setting up run eoqx1hgu (0.0s)
[Am[2K[34m[1mwandb[0m: setting up run eoqx1hgu (0.0s)
[Am[2K[34m[1mwandb[0m: setting up run eoqx1hgu (0.0s)
[Am[2K[34m[1mwandb[0m: setting up run eoqx1hgu (0.5s)
[Am[2K[34m[1mwandb[0m: Tracking run with wandb version 0.22.3
[34m[1mwandb[0m: Run data is saved locally in [35m[1m/home/manish/BrainTumorHFF/HFF/wandb/run-20251115_104603-eoqx1hgu[0m
[34m[1mwandb[0m: Run [1m`wandb offline`[0m to turn off syncing.
[34m[1mwandb[0m: Syncing run [33mglorious-shape-43[0m
[34m[1mwandb[0m:  View project at [34m[4mhttps://wandb.ai/k7aditya-iiita-alumni-affairs/learning%20rate%3D0.3epochs%3D200-step_size%3D50-gamma%3D0.55weight%20between%20braches%3D15warmup%20epochs%3D3brats20_all[0m
[34m[1mwandb[0m:  View run at [34m[4mhttps://wandb.ai/k7aditya-iiita-alumni-affairs/learning%20rate%3D0.3epochs%3D200-step_size%3D50-gamma%3D0.55weight%20between%20braches%3D15warmup%20epochs%3D3brats20_all/runs/eoqx1hgu[0m
[*] Load /home/manish/BrainTumorHFF/HFF/data/brats20/final/0-train.txt, which contains 295 paired volume
modilities: ['flair_L', 't1_L', 't1ce_L', 't2_L', 'flair_H1', 'flair_H2', 'flair_H3', 'flair_H4', 't1_H1', 't1_H2', 't1_H3', 't1_H4', 't1ce_H1', 't1ce_H2', 't1ce_H3', 't1ce_H4', 't2_H1', 't2_H2', 't2_H3', 't2_H4']
[*] Load /home/manish/BrainTumorHFF/HFF/data/brats20/final/0-val.txt, which contains 36 paired volume
modilities: ['flair_L', 't1_L', 't1ce_L', 't2_L', 'flair_H1', 'flair_H2', 'flair_H3', 'flair_H4', 't1_H1', 't1_H2', 't1_H3', 't1_H4', 't1ce_H1', 't1ce_H2', 't1ce_H3', 't1ce_H4', 't2_H1', 't2_H2', 't2_H3', 't2_H4']
[LOAD] Loading from: /home/manish/BrainTumorHFF/HFF/result/checkpoints/hff/brats20/all/hff-l=0.3-e=200-s=50-g=0.55-b=1-cw=15-w=3-100L-H-2025-11-14_11-06-41/best_Result1_et_0.7482_tc_0.7259_wt_0.7908.pth
[LOAD] âœ“ Checkpoint loaded successfully
[LOAD] Loading from: /home/manish/BrainTumorHFF/HFF/result/checkpoints/hff/brats20/all/hff-l=0.3-e=200-s=50-g=0.55-b=1-cw=15-w=3-100L-H-2025-11-14_11-06-41/best_Result1_et_0.7482_tc_0.7259_wt_0.7908.pth
[LOAD] âœ“ Checkpoint loaded successfully
[LOAD] Loading from: /home/manish/BrainTumorHFF/HFF/result/checkpoints/hff/brats20/all/hff-l=0.3-e=200-s=50-g=0.55-b=1-cw=15-w=3-100L-H-2025-11-14_11-06-41/best_Result1_et_0.7482_tc_0.7259_wt_0.7908.pth
[LOAD] âœ“ Checkpoint loaded successfully
Set dropout rate to 0.5000 (18 layers)
Set dropout rate to 0.2000 (18 layers)
Epoch 182 - Dropout rate set to 0.1380
  0%|                                                                                                                                                    | 0/99 [00:00<?, ?it/s]Set dropout rate to 0.5000 (18 layers)
Set dropout rate to 0.2000 (18 layers)
Set dropout rate to 0.5000 (18 layers)
Set dropout rate to 0.2000 (18 layers)
Traceback (most recent call last):
  File "/home/manish/BrainTumorHFF/HFF/train_multiGPU_fix.py", line 471, in <module>
    outputs_train_1, outputs_train_2,side1,side2 = model(low_freq_inputs, high_freq_inputs)
  File "/home/manish/.conda/envs/hff-xai/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/manish/.conda/envs/hff-xai/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1156, in forward
    output = self._run_ddp_forward(*inputs, **kwargs)
  File "/home/manish/.conda/envs/hff-xai/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1110, in _run_ddp_forward
    return module_to_run(*inputs[0], **kwargs[0])  # type: ignore[index]
  File "/home/manish/.conda/envs/hff-xai/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/manish/BrainTumorHFF/HFF/model/HFF_MobileNetV3_fixed.py", line 438, in forward
    layer1_LF, layer2_LF, layer3_LF, layer4_LF_1, layer5_LF_1 = self.mobilenet_encoder_b1(input1)
  File "/home/manish/.conda/envs/hff-xai/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/manish/BrainTumorHFF/HFF/model/HFF_MobileNetV3_fixed.py", line 104, in forward
    f1 = self.enc1(x)
  File "/home/manish/.conda/envs/hff-xai/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/manish/BrainTumorHFF/HFF/model/HFF_MobileNetV3_fixed.py", line 84, in forward
    return x + self.conv(x)
  File "/home/manish/.conda/envs/hff-xai/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/manish/.conda/envs/hff-xai/lib/python3.10/site-packages/torch/nn/modules/container.py", line 217, in forward
    input = module(input)
  File "/home/manish/.conda/envs/hff-xai/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/manish/.conda/envs/hff-xai/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py", line 74, in forward
    return self._apply_instance_norm(input)
  File "/home/manish/.conda/envs/hff-xai/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py", line 34, in _apply_instance_norm
    return F.instance_norm(
  File "/home/manish/.conda/envs/hff-xai/lib/python3.10/site-packages/torch/nn/functional.py", line 2495, in instance_norm
    return torch.instance_norm(
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 128.00 MiB (GPU 1; 23.70 GiB total capacity; 1.13 GiB already allocated; 23.56 MiB free; 1.14 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "/home/manish/BrainTumorHFF/HFF/train_multiGPU_fix.py", line 471, in <module>
    outputs_train_1, outputs_train_2,side1,side2 = model(low_freq_inputs, high_freq_inputs)
  File "/home/manish/.conda/envs/hff-xai/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/manish/.conda/envs/hff-xai/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1156, in forward
    output = self._run_ddp_forward(*inputs, **kwargs)
  File "/home/manish/.conda/envs/hff-xai/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1110, in _run_ddp_forward
    return module_to_run(*inputs[0], **kwargs[0])  # type: ignore[index]
  File "/home/manish/.conda/envs/hff-xai/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/manish/BrainTumorHFF/HFF/model/HFF_MobileNetV3_fixed.py", line 438, in forward
    layer1_LF, layer2_LF, layer3_LF, layer4_LF_1, layer5_LF_1 = self.mobilenet_encoder_b1(input1)
  File "/home/manish/.conda/envs/hff-xai/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/manish/BrainTumorHFF/HFF/model/HFF_MobileNetV3_fixed.py", line 104, in forward
    f1 = self.enc1(x)
  File "/home/manish/.conda/envs/hff-xai/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/manish/BrainTumorHFF/HFF/model/HFF_MobileNetV3_fixed.py", line 84, in forward
    return x + self.conv(x)
  File "/home/manish/.conda/envs/hff-xai/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/manish/.conda/envs/hff-xai/lib/python3.10/site-packages/torch/nn/modules/container.py", line 217, in forward
    input = module(input)
  File "/home/manish/.conda/envs/hff-xai/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/manish/.conda/envs/hff-xai/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py", line 74, in forward
    return self._apply_instance_norm(input)
  File "/home/manish/.conda/envs/hff-xai/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py", line 34, in _apply_instance_norm
    return F.instance_norm(
  File "/home/manish/.conda/envs/hff-xai/lib/python3.10/site-packages/torch/nn/functional.py", line 2495, in instance_norm
    return torch.instance_norm(
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 128.00 MiB (GPU 2; 23.70 GiB total capacity; 1.13 GiB already allocated; 23.56 MiB free; 1.14 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
