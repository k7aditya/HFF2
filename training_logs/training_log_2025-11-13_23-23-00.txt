[34m[1mwandb[0m: Currently logged in as: [33mk7aditya[0m ([33mk7aditya-iiita-alumni-affairs[0m) to [32mhttps://api.wandb.ai[0m. Use [1m`wandb login --relogin`[0m to force relogin
[34m[1mwandb[0m: setting up run drk155fx (0.0s)
[Am[2K[34m[1mwandb[0m: setting up run drk155fx (0.0s)
[Am[2K[34m[1mwandb[0m: setting up run drk155fx (0.0s)
[Am[2K[34m[1mwandb[0m: setting up run drk155fx (0.0s)
[Am[2K[34m[1mwandb[0m: setting up run drk155fx (0.0s)
[Am[2K[34m[1mwandb[0m: setting up run drk155fx (0.5s)
[Am[2K[34m[1mwandb[0m: Tracking run with wandb version 0.22.3
[34m[1mwandb[0m: Run data is saved locally in [35m[1m/home/manish/BrainTumorHFF/HFF/wandb/run-20251113_232301-drk155fx[0m
[34m[1mwandb[0m: Run [1m`wandb offline`[0m to turn off syncing.
[34m[1mwandb[0m: Syncing run [33msuper-sound-1[0m
[34m[1mwandb[0m:  View project at [34m[4mhttps://wandb.ai/k7aditya-iiita-alumni-affairs/HFF_MultiGPU_brats20_all[0m
[34m[1mwandb[0m:  View run at [34m[4mhttps://wandb.ai/k7aditya-iiita-alumni-affairs/HFF_MultiGPU_brats20_all/runs/drk155fx[0m

===============================================================
HFF-NET MULTI-GPU TRAINING
===============================================================
Rank: 0/4
Device: cuda:0
GPUs: 4
Log file: training_logs/training_log_2025-11-13_23-23-00.txt
Models saved to: ./result/checkpoints/hff/brats20/all/hff-l=0.3-e=200-s=50-g=0.55-b=1-cw=15-w=3-100L-H-2025-11-13_23-23-00

[RESUME INFO]
Checkpoint: /home/manish/BrainTumorHFF/HFF/result/checkpoints/hff/brats20/all/hff-l=0.3-e=200-s=50-g=0.55-b=1-cw=15-w=3-100L-H-2025-11-13_13-25-39/best_Result2_et_0.7411_tc_0.7448_wt_0.7826.pth
Resume epoch: 61
[*] Load /home/manish/BrainTumorHFF/HFF/data/brats20/final/0-train.txt, which contains 295 paired volume
modilities: ['flair_L', 't1_L', 't1ce_L', 't2_L', 'flair_H1', 'flair_H2', 'flair_H3', 'flair_H4', 't1_H1', 't1_H2', 't1_H3', 't1_H4', 't1ce_H1', 't1ce_H2', 't1ce_H3', 't1ce_H4', 't2_H1', 't2_H2', 't2_H3', 't2_H4']
[*] Load /home/manish/BrainTumorHFF/HFF/data/brats20/final/0-val.txt, which contains 36 paired volume
modilities: ['flair_L', 't1_L', 't1ce_L', 't2_L', 'flair_H1', 'flair_H2', 'flair_H3', 'flair_H4', 't1_H1', 't1_H2', 't1_H3', 't1_H4', 't1ce_H1', 't1ce_H2', 't1ce_H3', 't1ce_H4', 't2_H1', 't2_H2', 't2_H3', 't2_H4']
âœ“ Data loaded: 295 train batches, 36 val batches
âœ“ Model wrapped with DistributedDataParallel
Set dropout rate to 0.5000 (18 layers)

[RESUME] Loading checkpoint from: /home/manish/BrainTumorHFF/HFF/result/checkpoints/hff/brats20/all/hff-l=0.3-e=200-s=50-g=0.55-b=1-cw=15-w=3-100L-H-2025-11-13_13-25-39/best_Result2_et_0.7411_tc_0.7448_wt_0.7826.pth
[ERROR] Failed to load checkpoint: 'model_state_dict'
[ERROR] Starting training from scratch

Set dropout rate to 0.5000 (18 layers)
Epoch 1 - Dropout rate: 0.5000
  0%|                                                                  | 0/295 [00:00<?, ?it/s]  0%|                                                                  | 0/295 [00:17<?, ?it/s]
Traceback (most recent call last):
  File "/home/manish/BrainTumorHFF/HFF/train_new.py", line 476, in <module>
    outputs_train_1, outputs_train_2, side1, side2 = model(low_freq_inputs, high_freq_inputs)
  File "/home/manish/.conda/envs/hff-xai/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/manish/.conda/envs/hff-xai/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1156, in forward
    output = self._run_ddp_forward(*inputs, **kwargs)
  File "/home/manish/.conda/envs/hff-xai/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1110, in _run_ddp_forward
    return module_to_run(*inputs[0], **kwargs[0])  # type: ignore[index]
  File "/home/manish/.conda/envs/hff-xai/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/manish/BrainTumorHFF/HFF/model/HFF_MobileNetV3_fixed.py", line 438, in forward
    layer1_LF, layer2_LF, layer3_LF, layer4_LF_1, layer5_LF_1 = self.mobilenet_encoder_b1(input1)
  File "/home/manish/.conda/envs/hff-xai/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/manish/BrainTumorHFF/HFF/model/HFF_MobileNetV3_fixed.py", line 103, in forward
    x = self.stem(x)
  File "/home/manish/.conda/envs/hff-xai/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/manish/.conda/envs/hff-xai/lib/python3.10/site-packages/torch/nn/modules/container.py", line 217, in forward
    input = module(input)
  File "/home/manish/.conda/envs/hff-xai/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/manish/BrainTumorHFF/HFF/model/HFF_MobileNetV3_fixed.py", line 26, in forward
    return x * F.relu6(x + 3., inplace=self.inplace) / 6.
  File "/home/manish/.conda/envs/hff-xai/lib/python3.10/site-packages/torch/nn/functional.py", line 1532, in relu6
    result = torch._C._nn.relu6_(input)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 128.00 MiB (GPU 0; 23.70 GiB total capacity; 771.92 MiB already allocated; 89.56 MiB free; 820.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
