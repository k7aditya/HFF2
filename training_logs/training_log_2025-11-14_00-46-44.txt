[34m[1mwandb[0m: Currently logged in as: [33mk7aditya[0m ([33mk7aditya-iiita-alumni-affairs[0m) to [32mhttps://api.wandb.ai[0m. Use [1m`wandb login --relogin`[0m to force relogin
[34m[1mwandb[0m: setting up run ijdyny7n (0.0s)
[Am[2K[34m[1mwandb[0m: setting up run ijdyny7n (0.0s)
[Am[2K[34m[1mwandb[0m: setting up run ijdyny7n (0.0s)
[Am[2K[34m[1mwandb[0m: setting up run ijdyny7n (0.0s)
[Am[2K[34m[1mwandb[0m: setting up run ijdyny7n (0.0s)
[Am[2K[34m[1mwandb[0m: Tracking run with wandb version 0.22.3
[34m[1mwandb[0m: Run data is saved locally in [35m[1m/home/manish/BrainTumorHFF/HFF/wandb/run-20251114_004645-ijdyny7n[0m
[34m[1mwandb[0m: Run [1m`wandb offline`[0m to turn off syncing.
[34m[1mwandb[0m: Syncing run [33mglowing-rain-18[0m
[34m[1mwandb[0m:  View project at [34m[4mhttps://wandb.ai/k7aditya-iiita-alumni-affairs/learning%20rate%3D0.3epochs%3D200-step_size%3D50-gamma%3D0.55weight%20between%20braches%3D15warmup%20epochs%3D3brats20_all[0m
[34m[1mwandb[0m:  View run at [34m[4mhttps://wandb.ai/k7aditya-iiita-alumni-affairs/learning%20rate%3D0.3epochs%3D200-step_size%3D50-gamma%3D0.55weight%20between%20braches%3D15warmup%20epochs%3D3brats20_all/runs/ijdyny7n[0m
[*] Load /home/manish/BrainTumorHFF/HFF/data/brats20/final/0-train.txt, which contains 295 paired volume
modilities: ['flair_L', 't1_L', 't1ce_L', 't2_L', 'flair_H1', 'flair_H2', 'flair_H3', 'flair_H4', 't1_H1', 't1_H2', 't1_H3', 't1_H4', 't1ce_H1', 't1ce_H2', 't1ce_H3', 't1ce_H4', 't2_H1', 't2_H2', 't2_H3', 't2_H4']
[*] Load /home/manish/BrainTumorHFF/HFF/data/brats20/final/0-val.txt, which contains 36 paired volume
modilities: ['flair_L', 't1_L', 't1ce_L', 't2_L', 'flair_H1', 'flair_H2', 'flair_H3', 'flair_H4', 't1_H1', 't1_H2', 't1_H3', 't1_H4', 't1ce_H1', 't1ce_H2', 't1ce_H3', 't1ce_H4', 't2_H1', 't2_H2', 't2_H3', 't2_H4']
Set dropout rate to 0.5000 (18 layers)
Set dropout rate to 0.3780 (18 layers)
Epoch 62 - Dropout rate set to 0.3780
  0%|                                                                   | 0/295 [00:00<?, ?it/s]  0%|                                                                   | 0/295 [00:13<?, ?it/s]
Traceback (most recent call last):
  File "/home/manish/BrainTumorHFF/HFF/train_new.py", line 432, in <module>
    outputs_train_1, outputs_train_2,side1,side2 = model(low_freq_inputs, high_freq_inputs)
  File "/home/manish/.conda/envs/hff-xai/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/manish/BrainTumorHFF/HFF/model/HFF_MobileNetV3_fixed.py", line 438, in forward
    layer1_LF, layer2_LF, layer3_LF, layer4_LF_1, layer5_LF_1 = self.mobilenet_encoder_b1(input1)
  File "/home/manish/.conda/envs/hff-xai/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/manish/BrainTumorHFF/HFF/model/HFF_MobileNetV3_fixed.py", line 103, in forward
    x = self.stem(x)
  File "/home/manish/.conda/envs/hff-xai/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/manish/.conda/envs/hff-xai/lib/python3.10/site-packages/torch/nn/modules/container.py", line 217, in forward
    input = module(input)
  File "/home/manish/.conda/envs/hff-xai/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/manish/BrainTumorHFF/HFF/model/HFF_MobileNetV3_fixed.py", line 26, in forward
    return x * F.relu6(x + 3., inplace=self.inplace) / 6.
  File "/home/manish/.conda/envs/hff-xai/lib/python3.10/site-packages/torch/nn/functional.py", line 1532, in relu6
    result = torch._C._nn.relu6_(input)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 128.00 MiB (GPU 0; 23.70 GiB total capacity; 669.92 MiB already allocated; 51.56 MiB free; 718.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
